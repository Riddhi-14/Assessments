{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhfJmaCVQv6SzXY4+D2pg/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riddhi-14/Assessments/blob/main/CodingAssignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self):\n",
        "        self.tree = None\n",
        "\n",
        "    # Calculate entropy\n",
        "    def entropy(self, y):\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "        return entropy\n",
        "\n",
        "    # Calculate information gain\n",
        "    def information_gain(self, X, y, feature_idx, split_value):\n",
        "        left_mask = X[:, feature_idx] <= split_value\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_entropy = self.entropy(y[left_mask])\n",
        "        right_entropy = self.entropy(y[right_mask])\n",
        "\n",
        "        n = len(y)\n",
        "        left_weight = np.sum(left_mask) / n\n",
        "        right_weight = np.sum(right_mask) / n\n",
        "\n",
        "        information_gain = self.entropy(y) - (left_weight * left_entropy + right_weight * right_entropy)\n",
        "        return information_gain\n",
        "\n",
        "    # Find the best split for a given feature\n",
        "    def find_best_split(self, X, y, feature_idx):\n",
        "        unique_values = np.unique(X[:, feature_idx])\n",
        "        splits = []\n",
        "\n",
        "        for value in unique_values:\n",
        "            information_gain = self.information_gain(X, y, feature_idx, value)\n",
        "            splits.append([feature_idx, value, information_gain])\n",
        "\n",
        "        return splits\n",
        "\n",
        "    # Recursively build the decision tree\n",
        "    def build_tree(self, X, y, depth=0, max_depth=None):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "\n",
        "        if n_classes == 1 or (max_depth is not None and depth == max_depth):\n",
        "            return (None, None, None, np.argmax(np.bincount(y)))\n",
        "\n",
        "        best_feature_idx = None\n",
        "        best_split_value = None\n",
        "        best_information_gain = -1\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            splits = self.find_best_split(X, y, feature_idx)\n",
        "            for split in splits:\n",
        "                if split[2] > best_information_gain:\n",
        "                    best_feature_idx = split[0]\n",
        "                    best_split_value = split[1]\n",
        "                    best_information_gain = split[2]\n",
        "\n",
        "        if best_information_gain == 0:\n",
        "            return (None, None, None, np.argmax(np.bincount(y)))\n",
        "\n",
        "        left_mask = X[:, best_feature_idx] <= best_split_value\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_subtree = self.build_tree(X[left_mask], y[left_mask], depth + 1, max_depth)\n",
        "        right_subtree = self.build_tree(X[right_mask], y[right_mask], depth + 1, max_depth)\n",
        "\n",
        "        return (best_feature_idx, best_split_value, left_subtree, right_subtree)\n",
        "\n",
        "    # Fit the decision tree\n",
        "    def fit(self, X, y, max_depth=None):\n",
        "        self.tree = self.build_tree(X, y, max_depth=max_depth)\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict(self, x, tree):\n",
        "        if tree[0] is None:  # Base case: leaf node\n",
        "            return tree[3]  # Return the predicted class label\n",
        "        feature_idx, split_value, left_subtree, right_subtree = tree\n",
        "        if x[feature_idx] <= split_value:\n",
        "            return self._predict(x, left_subtree)\n",
        "        else:\n",
        "            return self._predict(x, right_subtree)\n",
        "\n",
        "# Mock training data\n",
        "X_train = np.array([[5.1, 3.5], [4.9, 3.0], [4.7, 3.2], [4.6, 3.1], [5.0, 3.6]])\n",
        "y_train = np.array([0, 0, 1, 1, 2])  # Example labels corresponding to the training data\n",
        "\n",
        "# Initialize the decision tree\n",
        "tree = DecisionTree()\n",
        "\n",
        "# Display information gain in a table\n",
        "splits = []\n",
        "for feature_idx in range(X_train.shape[1]):\n",
        "    feature_splits = tree.find_best_split(X_train, y_train, feature_idx)\n",
        "    splits.extend(feature_splits)\n",
        "\n",
        "print(tabulate(splits, headers=['Feature Index', 'Split Value', 'Information Gain'], tablefmt='grid'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VyVBRKWCVRZ",
        "outputId": "ca4b27ed-301a-4da6-e6ba-098e6435b5c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------+--------------------+\n",
            "|   Feature Index |   Split Value |   Information Gain |\n",
            "+=================+===============+====================+\n",
            "|               0 |           4.6 |           0.321928 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               0 |           4.7 |           0.970951 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               0 |           4.9 |           0.570951 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               0 |           5   |           0.321928 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               0 |           5.1 |           0        |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               1 |           3   |           0.321928 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               1 |           3.1 |           0.170951 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               1 |           3.2 |           0.570951 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               1 |           3.5 |           0.721928 |\n",
            "+-----------------+---------------+--------------------+\n",
            "|               1 |           3.6 |           0        |\n",
            "+-----------------+---------------+--------------------+\n"
          ]
        }
      ]
    }
  ]
}